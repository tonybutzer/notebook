{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Landsat Data in the Amazon Web Services (AWS) Cloud\n",
    "\n",
    "This tutorial demonstrates how users discover Landsat Data stored within the AWS Cloud environment by searching SAT-API. Landsat data stored in the AWS Cloud is located within U.S. West (Oregon) us-west-2 region in a requester pays Simple Storage Services (S3) bucket. Users interested in utilizing direct access to Landsat data stored in S3 are encouraged to visit the Requester Pays Documentation Page (https://docs.aws.amazon.com/AmazonS3/latest/dev/RequesterPaysBuckets.html) for information concerning potential egress costs to accessing Landsat data with this method. \n",
    "\n",
    "<b>AWS Bucket Name:</b> usgs-landsat (Requester Pays) <br />\n",
    "<b>AWS Region:</b> US West (Oregon) us-west-2<br />\n",
    "\n",
    "An AWS account is required before undertaking this tutorial. Please see the AWS Account website to establish an account (https://aws.amazon.com/account/).  <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search USGS Sat-API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import datetime\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "sat_api_url = \"https://landsatlook.usgs.gov/sat-api\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request SAT-API Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satAPI = requests.post(sat_api_url)\n",
    "if satAPI.status_code == 200:\n",
    "    print('Sat-API is Available')\n",
    "else:\n",
    "    print('Sat-API is not Available')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets see what was returned by the endpont\n",
    "print(json.dumps(satAPI.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at the collections\n",
    "satAPICollections = requests.post('https://landsatlook.usgs.gov/sat-api/collections')\n",
    "print(json.dumps(satAPICollections.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at the landsat-c2l2-sr collection\n",
    "c2L2SR = requests.post('https://landsatlook.usgs.gov/sat-api/collections/landsat-c2l2-sr')\n",
    "print(json.dumps(c2L2SR.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets pick a random product in the landsat-c2l2-sr Collection and look at the STAC Metadata\n",
    "srProduct = requests.post ('https://landsatlook.usgs.gov/sat-api/collections/landsat-c2l2-sr/items')\n",
    "print(json.dumps(srProduct.json()['features'][1]['geometry'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup a Sat-API Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will handle pagination\n",
    "def fetch_sat_api(query):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept-Encoding\": \"gzip\",\n",
    "        \"Accept\": \"application/geo+json\",\n",
    "    }\n",
    "\n",
    "    url = f\"{sat_api_url}/stac/search\"\n",
    "    data = requests.post(url, headers=headers, json=query).json()\n",
    "    error = data.get(\"message\", \"\")\n",
    "    if error:\n",
    "        raise Exception(f\"SAT-API failed and returned: {error}\")\n",
    "\n",
    "    meta = data.get(\"meta\", {})\n",
    "    if not meta.get(\"found\"):\n",
    "        return []\n",
    "    print(meta)\n",
    "\n",
    "    features = data[\"features\"]\n",
    "    if data[\"links\"]:\n",
    "        curr_page = int(meta[\"page\"])\n",
    "        query[\"page\"] = curr_page + 1\n",
    "        query[\"limit\"] = meta[\"limit\"]\n",
    "\n",
    "        features = list(itertools.chain(features, fetch_sat_api(query)))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up query\n",
    "#Find Scenes for Path 30, Row 28 in Collection 2 L2_SR Collection acquired between 1-January 1982 and 30-December 2019\n",
    "#where scene cloud cover is between 0 and 30\n",
    "min_cloud = 0\n",
    "max_cloud = 30\n",
    "\n",
    "date_min=\"1982-01-01\"\n",
    "date_max=\"2019-12-30\"\n",
    "\n",
    "start = datetime.datetime.strptime(date_min, \"%Y-%m-%d\").strftime(\"%Y-%m-%dT00:00:00Z\")\n",
    "end = datetime.datetime.strptime(date_max, \"%Y-%m-%d\").strftime(\"%Y-%m-%dT23:59:59Z\")\n",
    "\n",
    "query = {\n",
    "    \"time\": f\"{start}/{end}\",\n",
    "    \"query\": {\n",
    "        \"landsat:wrs_path\": {\"in\": [\"30\",\"030\"]}, #landsat Path and Row are not 0 padded so that is a bug we will need to fix\n",
    "        \"landsat:wrs_row\": {\"in\": [\"28\",\"028\"]}, #landsat Path and Row are not 0 padded so that is a bug we will need to fix\n",
    "        \"eo:cloud_cover\": {\"gte\": min_cloud, \"lt\": max_cloud},\n",
    "        \"collection\":{\"eq\": \"landsat-c2l2-sr\"}\n",
    "    },\n",
    "    \"limit\": 500 # We limit to 500 items per Page (requests) to make sure sat-api doesn't fail to return big features collection\n",
    "}\n",
    "\n",
    "features = fetch_sat_api(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Table to show results\n",
    "df = pd.DataFrame(features)\n",
    "#Add Acqusition Dates\n",
    "dates = []\n",
    "for item in df['properties']:\n",
    "    dates.append(item['datetime'])\n",
    "#Add Product IDs\n",
    "    ids = []\n",
    "for item in df['id']:\n",
    "    ids.append(item)\n",
    "#Add Green Bands\n",
    "#L8 has different band numbering for Green this accounts for that. Green = B3 L8 TM, ETM = B2\n",
    "greens = []\n",
    "for item in df['assets']:\n",
    "    if 'Green' in item['SR_B3.TIF']['title']: \n",
    "        greens.append(item['SR_B3.TIF']['href'])\n",
    "    else:\n",
    "        greens.append(item['SR_B2.TIF']['href'])\n",
    "#Add SWIR Bands \n",
    "#l8 SWIR Band = B6, L7 = B7, TM = B5 this accounts for that.\n",
    "        swirs = []\n",
    "for item in df['assets']:\n",
    "    try:\n",
    "        if 'Short-wave' in item['SR_B6.TIF']['title']:\n",
    "            swirs.append(item['SR_B6.TIF']['href'])\n",
    "    except:\n",
    "        if 'Short-wave' in item['SR_B5.TIF']['title']:\n",
    "            swirs.append(item['SR_B5.TIF']['href'])\n",
    "#Combine into one table\n",
    "ids = pd.DataFrame(ids, columns = ['Product ID'])\n",
    "dates = pd.DataFrame(dates, columns = ['Date'])\n",
    "greens = pd.DataFrame(greens, columns = ['Green'])\n",
    "swirs = pd.DataFrame(swirs, columns = ['Swirs'])\n",
    "df = pd.concat([ids, dates, greens, swirs], axis = 1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display DF, object links are the cloudfront links which will re-direct to ERS to login need to convert these to S3 object links\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove everything forward of /collection from the cloudfront object URL\n",
    "greens = pd.DataFrame(greens['Green'].str[33:])\n",
    "swirs = pd.DataFrame(swirs['Swirs'].str[33:])\n",
    "#append s3://usgs-landsat to the front of the URLs\n",
    "greens = pd.DataFrame(\"s3://usgs-landsat\"+greens['Green'])\n",
    "swirs = pd.DataFrame(\"s3://usgs-landsat\"+swirs['Swirs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new table with S3 links for objects\n",
    "df = pd.concat([ids, dates, greens, swirs], axis = 1, sort=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Date/Time Constructor to Date Column of DF\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a seasonality List to show only products where month acquired is May to September\n",
    "df = df[df['Date'].dt.month.between(5, 9)]\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose: Extracting an area of interest from a Landsat scene and visualizing change over time by using Xrarry and Holoviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask_kubernetes import KubeCluster\n",
    "# from dask.distributed import Client\n",
    "# from dask.distributed import wait, progress\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium import Marker\n",
    "import cartopy.crs as ccrs\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "import holoviews as hv\n",
    "import rasterio as rio\n",
    "from rasterio.session import AWSSession\n",
    "import boto3\n",
    "hv.extension('bokeh', width=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show scene extent against AOI. We will eventuly slice the array by the AOI to process only the data we need\n",
    "gf = features[1]['geometry']\n",
    "geoms = gpd.read_file('Waubay_AOI.geojson')\n",
    "m = folium.Map([46.0732, -97.7783], zoom_start=7, width=900, height=350, tiles='OpenStreetMap')\n",
    "folium.GeoJson(gf).add_to(m)\n",
    "folium.GeoJson(geoms).add_to(m)\n",
    "m.add_child(Marker(location=[45.3676, -97.4048], popup='Waubay, South Dakota AOI', icon = folium.Icon(color = 'red')))\n",
    "folium.LatLngPopup().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Kubernetes on the mini-pangeo - just use client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This allows us to use Dask with the requester pays bucket\n",
    "os.environ[\"AWS_REQUEST_PAYER\"] = \"requester\" \n",
    "#cluster = KubeCluster(env={'AWS_REQUEST_PAYER': 'requester'})\n",
    "# cluster\n",
    "#Click Manual Scaling\n",
    "#Add 10 to the Workers Field\n",
    "#Click Scale\n",
    "#Click the Dashboard Link and open in a new window to view compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_session = AWSSession(boto3.Session(), requester_pays=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract corners from AOI to use to subset array\n",
    "crs = ccrs.epsg(32614)\n",
    "bounds = geoms.geometry.bounds\n",
    "bounds.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_left_corner_lat_lon = bounds.minx.min(), bounds.miny.min()\n",
    "upper_right_corner_lat_lon = bounds.maxx.max(), bounds.maxy.max()\n",
    "\n",
    "print(lower_left_corner_lat_lon, upper_right_corner_lat_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_corner = crs.transform_point(*lower_left_corner_lat_lon, ccrs.PlateCarree())\n",
    "ur_corner = crs.transform_point(*upper_right_corner_lat_lon, ccrs.PlateCarree())\n",
    "\n",
    "print(ll_corner, ur_corner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used to create array from data stored in dataframe table created above\n",
    "def create_dataset(row, bands = ['Swirs', 'Green'], chunks = {'band': 1, 'x':2048, 'y':2049}):\n",
    "    datasets = []\n",
    "    with rio.Env(aws_session):\n",
    "        for band in bands:\n",
    "            url = row[band]\n",
    "            #da = xr.open_rasterio(url, chunks = chunks)\n",
    "            da = xr.open_rasterio(url)\n",
    "            daSub = da.sel(x=slice(ll_corner[0], ur_corner[0]), y=slice(ur_corner[1], ll_corner[1]))\n",
    "            daSub = daSub.squeeze().drop(labels='band')\n",
    "            DS = daSub.to_dataset(name = band)\n",
    "            datasets.append(DS)\n",
    "        DS = xr.merge(datasets)\n",
    "        return DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "datasets = []\n",
    "for i,row in df.iterrows():\n",
    "    try:\n",
    "        print('loading....', row.Date)\n",
    "        ds = create_dataset(row)\n",
    "        datasets.append(ds)\n",
    "    except Exception as e:\n",
    "        print('Error loading, skipping')\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = xr.concat(datasets, dim= pd.DatetimeIndex(df.Date.tolist(), name='time'))\n",
    "print('Dataset Size (Gb): ', DS.nbytes/1e9)\n",
    "DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate NDWI\n",
    "MNDWI = (DS['Green'] - DS['Swirs']) / (DS['Green'] + DS['Swirs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle\n",
    "\n",
    "with open('waubay_ndwi.pickle', 'wb') as handle:\n",
    "    pickle.dump(MNDWI, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = MNDWI.hvplot('x', 'y', groupby='time', dynamic=True, rasterize=True, width=700, height=500, cmap='BrBG')\n",
    "#use the time slider to step through time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install --user cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install --user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install --user dask_kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install --user pyepsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('drb150c_xarray.pickle', 'rb') as handle:\n",
    "#     da = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNDWI.to_netcdf('water.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! echo .nc >> .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
